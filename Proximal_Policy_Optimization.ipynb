{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing the important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VCKGYM7KF1tS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nconda install -c conda-forge swig -y\\nconda install -c conda-forge box2d-py -y\\npython -m pip install --upgrade pip setuptools wheel\\npython -m pip install gymnasium[box2d]\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "conda install -c conda-forge swig -y\n",
        "conda install -c conda-forge box2d-py -y\n",
        "python -m pip install --upgrade pip setuptools wheel\n",
        "python -m pip install gymnasium[box2d]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing the important modules\n",
        "\n",
        "It is preferred to train the model on a device with a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tCwaSKTwHC-r"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementation of the Car Racing game from Gymnasium using Proximal Policy Optimization\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/car_racing/#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Proximal Policy Optimization (PPO)?\n",
        "\n",
        "**Proximal Policy Optimization (PPO)** is a Reinforcement Learning (RL) algorithm used to train *agents* to perform tasks. Developed by OpenAI in 2017, it has become an industry standard because it strikes an excellent balance between:\n",
        "\n",
        "- Ease of implementation  \n",
        "- Sample efficiency  \n",
        "- Stability and ease of tuning  \n",
        "\n",
        "In the referenced notebook, PPO is used to train a car to drive in the `CarRacing-v3` environment.\n",
        "\n",
        "---\n",
        "\n",
        "## How PPO Works: Core Concepts\n",
        "\n",
        "PPO belongs to the family of **Policy Gradient** methods. Instead of learning a value table or Q-function, it directly learns a **policy**—a mapping from observations to actions.\n",
        "\n",
        "### 1. Actor–Critic Architecture\n",
        "\n",
        "The implementation uses two neural networks:\n",
        "\n",
        "- **Actor**  \n",
        "  - Takes the observation (processed pixels)\n",
        "  - Outputs a probability distribution over actions\n",
        "  - Responsible for *acting*\n",
        "\n",
        "- **Critic**  \n",
        "  - Takes the same observation\n",
        "  - Predicts the expected future reward (value)\n",
        "  - Helps evaluate how good the Actor’s actions were\n",
        "\n",
        "The Critic provides a learning signal to stabilize and guide the Actor.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The “Proximal” Idea: Clipped Objective Function\n",
        "\n",
        "A major challenge in policy gradient methods is that large updates can completely destabilize learning.\n",
        "\n",
        "PPO solves this using a **clipped surrogate objective**.\n",
        "\n",
        "#### Probability Ratio\n",
        "\n",
        "During training, PPO computes the ratio:\n",
        "\n",
        "```\n",
        "ratio = π_new(a|s) / π_old(a|s)\n",
        "```\n",
        "\n",
        "- `ratio = 1` → no change  \n",
        "- `ratio >> 1 or << 1` → update is too aggressive  \n",
        "\n",
        "#### Clipping\n",
        "\n",
        "The ratio is clipped to a fixed interval, typically:\n",
        "\n",
        "```\n",
        "[1 − ε, 1 + ε]\n",
        "```\n",
        "\n",
        "This prevents excessively large policy updates, ensuring **stable and monotonic improvement**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Generalized Advantage Estimation (GAE)\n",
        "\n",
        "The notebook uses a discounted return computation with a `lambda_` parameter. This corresponds to **Generalized Advantage Estimation (GAE)**.\n",
        "\n",
        "GAE balances:\n",
        "\n",
        "- **Low variance** (bootstrapping from the Critic)\n",
        "- **Low bias** (using actual rewards)\n",
        "\n",
        "It estimates the **advantage**:\n",
        "\n",
        "```\n",
        "Advantage = How much better an action was than expected\n",
        "```\n",
        "\n",
        "This improves learning stability and efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## Breakdown of the Implementation\n",
        "\n",
        "| Feature | Implementation Detail |\n",
        "|------|------------------------|\n",
        "| Observation | Frames resized to 84×84, grayscale, normalized |\n",
        "| Frame stacking | 4 frames stacked to capture motion |\n",
        "| Action space | Discrete actions (`action_dim = 5`) |\n",
        "| Exploration | `Categorical` action distribution |\n",
        "| Clipping | `self.clip = 0.4` |\n",
        "| Value loss | Mean Squared Error (MSE) |\n",
        "| Policy loss | PPO clipped objective |\n",
        "\n",
        "---\n",
        "\n",
        "## Training Loop Logic\n",
        "\n",
        "1. **Data Collection**  \n",
        "   - The agent interacts with the environment\n",
        "   - Observations, actions, rewards, and log-probabilities are stored\n",
        "\n",
        "2. **Advantage Computation**  \n",
        "   - The Critic estimates state values\n",
        "   - GAE computes advantages\n",
        "\n",
        "3. **Policy Optimization**  \n",
        "   - Actor is updated to increase probability of good actions\n",
        "   - Clipping prevents large destructive updates\n",
        "\n",
        "4. **Value Function Update**  \n",
        "   - Critic is trained to better predict returns\n",
        "\n",
        "---\n",
        "\n",
        "## Why PPO Works Well\n",
        "\n",
        "- Stable training\n",
        "- Few hyperparameters\n",
        "- Works well with high-dimensional inputs (images)\n",
        "- Robust to imperfect tuning\n",
        "\n",
        "These properties make PPO a strong choice for environments like **CarRacing-v2**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Preprocessing\n",
        "\n",
        "### Observation Space:\n",
        "- A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "To make learning efficient, we can't just throw raw RGB frames at the network. The following section handles State Representation:\n",
        "\n",
        "- **Preprocessing:** We convert images to grayscale and resize them to $84 \\times 84$ to reduce the computational load.\n",
        "- **Frame Skipping:** The agent doesn't need to see every single frame to understand movement. We skip frames to speed up training.\n",
        "- **Frame Stacking:** A single static image doesn't show velocity. By stacking the 4 most recent frames, we provide the network with temporal context (motion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u4E7hNxDHHy3"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img):\n",
        "  img = cv2.resize(img, dsize=(84, 84))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the environment wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6KIGPe4dHP1k"
      },
      "outputs": [],
      "source": [
        "class CarEnvironment(gym.Wrapper):\n",
        "  def __init__(self, env, skip_frames=4, stack_frames=4, no_operation=50, **kwargs):\n",
        "    super().__init__(env, **kwargs)\n",
        "    self._no_operation = no_operation\n",
        "    self._skip_frames = skip_frames\n",
        "    self._stack_frames = stack_frames\n",
        "\n",
        "  def reset(self):\n",
        "    observation, info = self.env.reset()\n",
        "\n",
        "    for i in range(self._no_operation):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
        "    return self.stack_state, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0\n",
        "    for i in range(self._skip_frames):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
        "    return self.stack_state, total_reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WGdemfVYMa4h"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self._n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self._n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self._n_features))\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self._n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self._n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self._n_features))\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vNhJODENE0Xc"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "  def __init__(self, action_dim=5, obs_dim=4, episodes=1500, trajectories=300, gamma=0.99, lr_actor=0.0001, lr_critic=0.0001, clip=0.4, n_updates=3, lambda_=0.99):\n",
        "    self.action_dim = action_dim\n",
        "    self.obs_dim = obs_dim\n",
        "    self.episodes = episodes\n",
        "    self.trajectories = trajectories\n",
        "    self.gamma = gamma\n",
        "    self.lr_actor = lr_actor\n",
        "    self.lr_critic = lr_critic\n",
        "    self.clip = clip\n",
        "    self.n_updates = n_updates\n",
        "    self.lambda_ = lambda_\n",
        "    self._total_rewards = []\n",
        "    self.actor = Actor(obs_dim, action_dim).to(device)\n",
        "    self.critic = Critic(obs_dim, 1).to(device)\n",
        "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr_actor)\n",
        "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr_critic)\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  This function takes as a parameter an observation, feeds it to the CNN and gets the\n",
        "  raw predictions (logits) and it samples an action through the categorical distribution.\n",
        "  It returns the action and the logarithmic probability.\n",
        "  \"\"\"\n",
        "  def get_action(self, obs):\n",
        "    # Our observation is a 2D numpy array so we first create a tensor\n",
        "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    # Feeding the tensor to the actor and get the logits\n",
        "    action_probs = self.actor(obs)\n",
        "\n",
        "    # Creating a Categorical distribution\n",
        "    dist = Categorical(logits=action_probs)\n",
        "\n",
        "    # Sampling the action\n",
        "    action = dist.sample()\n",
        "\n",
        "    log_prob = dist.log_prob(action)\n",
        "\n",
        "    return action.detach().cpu().numpy(), log_prob.detach()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  This function is where we collect the trajectories (e.g. observations, rewards and other information)\n",
        "  using the current policy. We run this until we collect the number of trajectories we set.\n",
        "  It returns all the collected information.\n",
        "  \"\"\"\n",
        "  def collect_trajectories(self):\n",
        "    batch_obs = []\n",
        "    batch_rewards = []\n",
        "    batch_log_probs = []\n",
        "    batch_next_obs = []\n",
        "    batch_actions = []\n",
        "    batch_dones = []\n",
        "    t = 0\n",
        "\n",
        "    # Creating the discrete environment and passing it through the our wrapper for the modification\n",
        "    env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "    env = CarEnvironment(env)\n",
        "\n",
        "    while True:\n",
        "\n",
        "      # Reset environment\n",
        "      obs, _ = env.reset()\n",
        "\n",
        "      # Runs as many times as needed until we get the number of trajectories we want\n",
        "      while True:\n",
        "\n",
        "        # Append current state\n",
        "        batch_obs.append(obs)\n",
        "\n",
        "        # Choose an action\n",
        "        a, log_prob = self.get_action(obs)\n",
        "\n",
        "        # Append action\n",
        "        batch_actions.append(a)\n",
        "\n",
        "        # Append log prob\n",
        "        batch_log_probs.append(log_prob)\n",
        "\n",
        "        # Perform the action\n",
        "        obs, rew, terminated, truncated, _ = env.step(a.item())\n",
        "\n",
        "        # Append reward\n",
        "        batch_rewards.append(rew)\n",
        "\n",
        "        # Increase the number of T horizon\n",
        "        t += 1\n",
        "\n",
        "        # Check criterion for loop termination\n",
        "        if terminated or truncated or t == self.trajectories:\n",
        "          batch_dones.append(1)\n",
        "          break\n",
        "        else:\n",
        "          batch_dones.append(0)\n",
        "\n",
        "      # Check criterion for loop termination\n",
        "      if t == self.trajectories:\n",
        "        env.close()\n",
        "        break\n",
        "\n",
        "    self._total_rewards.append(sum(batch_rewards))\n",
        "\n",
        "    # Convert to tensors\n",
        "    batch_obs = np.array(batch_obs)\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float32)\n",
        "    batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32)\n",
        "    batch_actions = torch.tensor(batch_actions, dtype=torch.long)\n",
        "\n",
        "    # Reward Normalization\n",
        "    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-8)\n",
        "\n",
        "    return batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Computing the discounted reward sum based on the current V values with\n",
        "  GAE (Generalized Advantage Estimation)\n",
        "  \"\"\"\n",
        "  def compute_discounted_sum(self, batch_rewards, V, batch_dones):\n",
        "    discounted_sum = []\n",
        "    gae = 0\n",
        "    zero = torch.tensor([0])\n",
        "    V = torch.cat((V.cpu(), zero))\n",
        "\n",
        "    for i in reversed(range(len(batch_rewards))):\n",
        "      delta = batch_rewards[i] + self.gamma * V[i + 1] * (1 - batch_dones[i]) - V[i]\n",
        "      gae = delta + self.gamma * self.lambda_ * gae * (1 - batch_dones[i])\n",
        "      discounted_sum.insert(0, gae)\n",
        "\n",
        "    return discounted_sum\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Make the agent learn the environment\n",
        "  \"\"\"\n",
        "  def train(self):\n",
        "\n",
        "    # Running the training phase for some episodes\n",
        "    for episode in range(self.episodes):\n",
        "\n",
        "      if episode % 10 == 0:\n",
        "        print(self._total_rewards)\n",
        "\n",
        "      if (1 + episode) % 50 == 0:\n",
        "        print(\"Processed: \", episode + 1)\n",
        "        print()\n",
        "        torch.save(model.actor.state_dict(), f'actor_weights_{episode + 1}.pth')\n",
        "        torch.save(model.critic.state_dict(), f'critic_weights_{episode + 1}.pth')\n",
        "        with open('statistics.pkl', 'wb') as f:\n",
        "          pickle.dump((self._total_rewards), f)\n",
        "\n",
        "      # Collecting the batches with the information\n",
        "      batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones = self.collect_trajectories()\n",
        "\n",
        "      # Compute V values with the critic network in current states\n",
        "      V = self.critic(batch_obs.to(device)).squeeze()\n",
        "\n",
        "      # Compute the discounted sum\n",
        "      discounted_sum = self.compute_discounted_sum(batch_rewards, V, batch_dones)\n",
        "      discounted_sum = torch.tensor(discounted_sum, dtype=torch.float32)\n",
        "\n",
        "      # The advantages to maximize\n",
        "      advantages = discounted_sum - V.detach().cpu()\n",
        "      advantages = advantages.clone()\n",
        "\n",
        "      # Normalizing the advantage sum to help the network from exploding gradients\n",
        "      advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "      # Update the network\n",
        "      for update in range(self.n_updates):\n",
        "\n",
        "        # Finding the probabilites of our collected observations\n",
        "        actions_probs = self.actor(batch_obs.to(device))\n",
        "\n",
        "        # Gathering the probabilities of the sampled actions (current policy through categorical sampling)\n",
        "        action_log_probs = actions_probs.gather(1, batch_actions.to(device)).squeeze()\n",
        "\n",
        "        # Computing the probability ratio between the new and old policy\n",
        "        ratios = torch.exp(action_log_probs - batch_log_probs.to(device)).cpu()\n",
        "\n",
        "        # Computing the normal policy gradient objective\n",
        "        surr1 = ratios * advantages\n",
        "\n",
        "        # Clipping the policy ratio\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
        "\n",
        "        # Picking the minimum of the two surrogate objects and adding minus so we can create a minimization problem\n",
        "        loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "        # Back propagation, returning the losses for both actor & critic networks\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        V = self.critic(batch_obs.to(device)).squeeze()\n",
        "        value_loss = nn.MSELoss()(V, discounted_sum.detach().to(device))\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.critic_optim.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8HaPx_du3Rd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/rl_gymnasium/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/tmp/ipykernel_3487809/2009179959.py:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  batch_actions = torch.tensor(batch_actions, dtype=torch.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215]\n",
            "Processed:  50\n",
            "\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831, -12.126089217012321, 36.88711656441635, -43.30469756807471, 29.195286529396306, -35.26983482467418, -21.237757996125474, 88.01561913979711, 44.48094457597142, -23.947826086956006, 22.84893642648838]\n",
            "Processed:  100\n",
            "\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831, -12.126089217012321, 36.88711656441635, -43.30469756807471, 29.195286529396306, -35.26983482467418, -21.237757996125474, 88.01561913979711, 44.48094457597142, -23.947826086956006, 22.84893642648838, -91.00912881281653, -62.812439388856845, -38.0235049766691, -103.25314637895832, -183.12659296791182, -55.268840046271954, -152.13449724721508, -90.92590799031626, -132.2406979039213, -65.86519430377598]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831, -12.126089217012321, 36.88711656441635, -43.30469756807471, 29.195286529396306, -35.26983482467418, -21.237757996125474, 88.01561913979711, 44.48094457597142, -23.947826086956006, 22.84893642648838, -91.00912881281653, -62.812439388856845, -38.0235049766691, -103.25314637895832, -183.12659296791182, -55.268840046271954, -152.13449724721508, -90.92590799031626, -132.2406979039213, -65.86519430377598, -118.91755048988145, -156.4851531327523, 14.553018930273314, -127.90564423413048, -93.42401906857492, -137.07042928657833, -72.41176067061058, -138.50784888047625, -121.28605358160888, 27.90677328185908]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831, -12.126089217012321, 36.88711656441635, -43.30469756807471, 29.195286529396306, -35.26983482467418, -21.237757996125474, 88.01561913979711, 44.48094457597142, -23.947826086956006, 22.84893642648838, -91.00912881281653, -62.812439388856845, -38.0235049766691, -103.25314637895832, -183.12659296791182, -55.268840046271954, -152.13449724721508, -90.92590799031626, -132.2406979039213, -65.86519430377598, -118.91755048988145, -156.4851531327523, 14.553018930273314, -127.90564423413048, -93.42401906857492, -137.07042928657833, -72.41176067061058, -138.50784888047625, -121.28605358160888, 27.90677328185908, -173.53242926898588, -92.97247036340258, -88.35048041389261, 107.05677803788447, 0.5354399008680897, 15.40515893397161, -103.45709675014172, -3.829927760578755, -83.04132898903333, 8.745367675801681]\n",
            "[-47.24769354938889, -50.826775210578404, -39.63215547703245, -41.738986063934576, -76.53374613003139, -59.74112736698834, -31.534880136292188, -52.10745906610115, -37.719728660259875, -50.243036282452366, -43.538031771836366, -22.770550576184363, -23.032258136201577, -65.17491132290876, 12.645520581113015, -61.02279495990902, -38.00756288522448, -61.428649635036884, -30.389308104411384, -26.525646879756472, -29.967310167310323, -44.392846721085746, -28.931549536520244, -74.68689714041906, -32.94920965780834, 13.199822390621243, -31.204811404811586, -102.52564892734495, -1.3538847117801183, -35.97989853284039, -38.96808294716777, 13.31250153355344, 4.843532314395769, 44.49463171036342, 4.445169082124668, 22.547547185327282, -2.6193212223637485, 28.795813132553324, -0.12360831656669724, -105.61081081081215, 39.87385295743374, -43.24204562854136, 27.14649842710653, -39.00532998599975, -147.63171521035702, 68.49485121619868, -43.54127521946812, -142.84640617611, -0.8377247620173449, -176.34512878426057, -86.56802072980605, -194.01006663933612, -160.00976178810703, -70.7096009363291, -130.68930273456488, -233.67410702844072, -81.09098546681551, 57.13756429052699, -149.30094372969188, -101.11541764730299, -150.58615332658016, -102.50082985312405, 30.96749223090579, -7.755084391129259, 55.20318187603321, 76.96558743124478, 57.20372786579736, -121.1230498344647, 9.37533084125535, 48.50944720678503, -98.61785714285816, -97.0538193233356, -130.12473451919624, -183.9013368389239, -76.4295357621557, 153.72321381409364, 6.103826631030628, 13.726906697637443, 111.46873126873402, 4.890504059776831, -12.126089217012321, 36.88711656441635, -43.30469756807471, 29.195286529396306, -35.26983482467418, -21.237757996125474, 88.01561913979711, 44.48094457597142, -23.947826086956006, 22.84893642648838, -91.00912881281653, -62.812439388856845, -38.0235049766691, -103.25314637895832, -183.12659296791182, -55.268840046271954, -152.13449724721508, -90.92590799031626, -132.2406979039213, -65.86519430377598, -118.91755048988145, -156.4851531327523, 14.553018930273314, -127.90564423413048, -93.42401906857492, -137.07042928657833, -72.41176067061058, -138.50784888047625, -121.28605358160888, 27.90677328185908, -173.53242926898588, -92.97247036340258, -88.35048041389261, 107.05677803788447, 0.5354399008680897, 15.40515893397161, -103.45709675014172, -3.829927760578755, -83.04132898903333, 8.745367675801681, 122.89301470588238, -67.64388001935137, 123.85591397849797, 9.365900667160997, -36.13056544088421, 138.24040756334, -30.734758990150794, 71.34082219168155, -81.40555555555702, 18.516664178732984]\n"
          ]
        }
      ],
      "source": [
        "model = PPO()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XMyVoXmBAGRr"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "eval_env = CarEnvironment(eval_env)\n",
        "\n",
        "frames = []\n",
        "scores = 0\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "done, ret = False, 0\n",
        "\n",
        "while not done:\n",
        "    frames.append(eval_env.render())\n",
        "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    a = torch.argmax(model.actor(s), dim=-1)\n",
        "    discrete_action = a.item() % 5\n",
        "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
        "    s = s_prime\n",
        "    ret += r\n",
        "    done = terminated or truncated\n",
        "    if terminated:\n",
        "      print(terminated)\n",
        "scores += ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQxJLASIAkSU"
      },
      "outputs": [],
      "source": [
        "def animate(imgs, video_name, _return=True):\n",
        "    import cv2\n",
        "    import os\n",
        "    import string\n",
        "    import random\n",
        "\n",
        "    if video_name is None:\n",
        "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
        "    height, width, layers = imgs[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
        "\n",
        "    for img in imgs:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        video.write(img)\n",
        "    video.release()\n",
        "\n",
        "animate(frames, None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl_gymnasium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
