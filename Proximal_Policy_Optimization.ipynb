{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCKGYM7KF1tS"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCwaSKTwHC-r"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4E7hNxDHHy3"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img):\n",
        "  img = cv2.resize(img, dsize=(84, 84))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KIGPe4dHP1k"
      },
      "outputs": [],
      "source": [
        "class CarEnvironment(gym.Wrapper):\n",
        "  def __init__(self, env, skip_frames=4, stack_frames=4, no_operation=50, **kwargs):\n",
        "    super().__init__(env, **kwargs)\n",
        "    self._no_operation = no_operation\n",
        "    self._skip_frames = skip_frames\n",
        "    self._stack_frames = stack_frames\n",
        "\n",
        "  def reset(self):\n",
        "    observation, info = self.env.reset()\n",
        "\n",
        "    for i in range(self._no_operation):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
        "    return self.stack_state, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0\n",
        "    for i in range(self._skip_frames):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
        "    return self.stack_state, total_reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGdemfVYMa4h"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self._n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self._n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self._n_features))\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self._n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self._n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self._n_features))\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNhJODENE0Xc"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "  def __init__(self, action_dim=5, obs_dim=4, episodes=1500, trajectories=300, gamma=0.99, lr_actor=0.0001, lr_critic=0.0001, clip=0.4, n_updates=3, lambda_=0.99):\n",
        "    self.action_dim = action_dim\n",
        "    self.obs_dim = obs_dim\n",
        "    self.episodes = episodes\n",
        "    self.trajectories = trajectories\n",
        "    self.gamma = gamma\n",
        "    self.lr_actor = lr_actor\n",
        "    self.lr_critic = lr_critic\n",
        "    self.clip = clip\n",
        "    self.n_updates = n_updates\n",
        "    self.lambda_ = lambda_\n",
        "    self._total_rewards = []\n",
        "    self.actor = Actor(obs_dim, action_dim).to(device)\n",
        "    self.critic = Critic(obs_dim, 1).to(device)\n",
        "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr_actor)\n",
        "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr_critic)\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  This function takes as a parameter an observation, feeds it to the CNN and gets the\n",
        "  raw predictions (logits) and it samples an action through the categorical distribution.\n",
        "  It returns the action and the logarithmic probability.\n",
        "  \"\"\"\n",
        "  def get_action(self, obs):\n",
        "    # Our observation is a 2D numpy array so we first create a tensor\n",
        "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    # Feeding the tensor to the actor and get the logits\n",
        "    action_probs = self.actor(obs)\n",
        "\n",
        "    # Creating a Categorical distribution\n",
        "    dist = Categorical(logits=action_probs)\n",
        "\n",
        "    # Sampling the action\n",
        "    action = dist.sample()\n",
        "\n",
        "    log_prob = dist.log_prob(action)\n",
        "\n",
        "    return action.detach().cpu().numpy(), log_prob.detach()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  This function is where we collect the trajectories (e.g. observations, rewards and other information)\n",
        "  using the current policy. We run this until we collect the number of trajectories we set.\n",
        "  It returns all the collected information.\n",
        "  \"\"\"\n",
        "  def collect_trajectories(self):\n",
        "    batch_obs = []\n",
        "    batch_rewards = []\n",
        "    batch_log_probs = []\n",
        "    batch_next_obs = []\n",
        "    batch_actions = []\n",
        "    batch_dones = []\n",
        "    t = 0\n",
        "\n",
        "    # Creating the discrete environment and passing it through the our wrapper for the modification\n",
        "    env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "    env = CarEnvironment(env)\n",
        "\n",
        "    while True:\n",
        "\n",
        "      # Reset environment\n",
        "      obs, _ = env.reset()\n",
        "\n",
        "      # Runs as many times as needed until we get the number of trajectories we want\n",
        "      while True:\n",
        "\n",
        "        # Append current state\n",
        "        batch_obs.append(obs)\n",
        "\n",
        "        # Choose an action\n",
        "        a, log_prob = self.get_action(obs)\n",
        "\n",
        "        # Append action\n",
        "        batch_actions.append(a)\n",
        "\n",
        "        # Append log prob\n",
        "        batch_log_probs.append(log_prob)\n",
        "\n",
        "        # Perform the action\n",
        "        obs, rew, terminated, truncated, _ = env.step(a.item())\n",
        "\n",
        "        # Append reward\n",
        "        batch_rewards.append(rew)\n",
        "\n",
        "        # Increase the number of T horizon\n",
        "        t += 1\n",
        "\n",
        "        # Check criterion for loop termination\n",
        "        if terminated or truncated or t == self.trajectories:\n",
        "          batch_dones.append(1)\n",
        "          break\n",
        "        else:\n",
        "          batch_dones.append(0)\n",
        "\n",
        "      # Check criterion for loop termination\n",
        "      if t == self.trajectories:\n",
        "        env.close()\n",
        "        break\n",
        "\n",
        "    self._total_rewards.append(sum(batch_rewards))\n",
        "\n",
        "    # Convert to tensors\n",
        "    batch_obs = np.array(batch_obs)\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float32)\n",
        "    batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32)\n",
        "    batch_actions = torch.tensor(batch_actions, dtype=torch.long)\n",
        "\n",
        "    # Reward Normalization\n",
        "    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-8)\n",
        "\n",
        "    return batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Computing the discounted reward sum based on the current V values with\n",
        "  GAE (Generalized Advantage Estimation)\n",
        "  \"\"\"\n",
        "  def compute_discounted_sum(self, batch_rewards, V, batch_dones):\n",
        "    discounted_sum = []\n",
        "    gae = 0\n",
        "    zero = torch.tensor([0])\n",
        "    V = torch.cat((V.cpu(), zero))\n",
        "\n",
        "    for i in reversed(range(len(batch_rewards))):\n",
        "      delta = batch_rewards[i] + self.gamma * V[i + 1] * (1 - batch_dones[i]) - V[i]\n",
        "      gae = delta + self.gamma * self.lambda_ * gae * (1 - batch_dones[i])\n",
        "      discounted_sum.insert(0, gae)\n",
        "\n",
        "    return discounted_sum\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Make the agent learn the environment\n",
        "  \"\"\"\n",
        "  def train(self):\n",
        "\n",
        "    # Running the training phase for some episodes\n",
        "    for episode in range(self.episodes):\n",
        "\n",
        "      if episode % 10 == 0:\n",
        "        print(self._total_rewards)\n",
        "\n",
        "      if (1 + episode) % 50 == 0:\n",
        "        print(\"Processed: \", episode + 1)\n",
        "        print()\n",
        "        torch.save(model.actor.state_dict(), f'actor_weights_{episode + 1}.pth')\n",
        "        torch.save(model.critic.state_dict(), f'critic_weights_{episode + 1}.pth')\n",
        "        with open('statistics.pkl', 'wb') as f:\n",
        "          pickle.dump((self._total_rewards), f)\n",
        "\n",
        "      # Collecting the batches with the information\n",
        "      batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones = self.collect_trajectories()\n",
        "\n",
        "      # Compute V values with the critic network in current states\n",
        "      V = self.critic(batch_obs.to(device)).squeeze()\n",
        "\n",
        "      # Compute the discounted sum\n",
        "      discounted_sum = self.compute_discounted_sum(batch_rewards, V, batch_dones)\n",
        "      discounted_sum = torch.tensor(discounted_sum, dtype=torch.float32)\n",
        "\n",
        "      # The advantages to maximize\n",
        "      advantages = discounted_sum - V.detach().cpu()\n",
        "      advantages = advantages.clone()\n",
        "\n",
        "      # Normalizing the advantage sum to help the network from exploding gradients\n",
        "      advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "      # Update the network\n",
        "      for update in range(self.n_updates):\n",
        "\n",
        "        # Finding the probabilites of our collected observations\n",
        "        actions_probs = self.actor(batch_obs.to(device))\n",
        "\n",
        "        # Gathering the probabilities of the sampled actions (current policy through categorical sampling)\n",
        "        action_log_probs = actions_probs.gather(1, batch_actions.to(device)).squeeze()\n",
        "\n",
        "        # Computing the probability ratio between the new and old policy\n",
        "        ratios = torch.exp(action_log_probs - batch_log_probs.to(device)).cpu()\n",
        "\n",
        "        # Computing the normal policy gradient objective\n",
        "        surr1 = ratios * advantages\n",
        "\n",
        "        # Clipping the policy ratio\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
        "\n",
        "        # Picking the minimum of the two surrogate objects and adding minus so we can create a minimization problem\n",
        "        loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "        # Back propagation, returning the losses for both actor & critic networks\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        V = self.critic(batch_obs.to(device)).squeeze()\n",
        "        value_loss = nn.MSELoss()(V, discounted_sum.detach().to(device))\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.critic_optim.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8HaPx_du3Rd5"
      },
      "outputs": [],
      "source": [
        "model = PPO()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XMyVoXmBAGRr"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "eval_env = CarEnvironment(eval_env)\n",
        "\n",
        "frames = []\n",
        "scores = 0\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "done, ret = False, 0\n",
        "\n",
        "while not done:\n",
        "    frames.append(eval_env.render())\n",
        "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    a = torch.argmax(model.actor(s), dim=-1)\n",
        "    discrete_action = a.item() % 5\n",
        "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
        "    s = s_prime\n",
        "    ret += r\n",
        "    done = terminated or truncated\n",
        "    if terminated:\n",
        "      print(terminated)\n",
        "scores += ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQxJLASIAkSU"
      },
      "outputs": [],
      "source": [
        "def animate(imgs, video_name, _return=True):\n",
        "    import cv2\n",
        "    import os\n",
        "    import string\n",
        "    import random\n",
        "\n",
        "    if video_name is None:\n",
        "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
        "    height, width, layers = imgs[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
        "\n",
        "    for img in imgs:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        video.write(img)\n",
        "    video.release()\n",
        "\n",
        "animate(frames, None)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}