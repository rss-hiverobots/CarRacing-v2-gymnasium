{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSPpKLU97MSm"
      },
      "source": [
        "# Installing the important libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pT7fTAWKXHl3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nconda install -c conda-forge swig -y\\nconda install -c conda-forge box2d-py -y\\npython -m pip install --upgrade pip setuptools wheel\\npython -m pip install gymnasium[box2d]\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "conda install -c conda-forge swig -y\n",
        "conda install -c conda-forge box2d-py -y\n",
        "python -m pip install --upgrade pip setuptools wheel\n",
        "python -m pip install gymnasium[box2d]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqrTw6l7Uis"
      },
      "source": [
        "# Importing the important modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G41GVG9LcXiv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhOwyKe47a3d"
      },
      "source": [
        "# Implementation of the Car Racing game from Gymnasium using Deep Q Learning with CNN\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/car_racing/#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Preprocessing\n",
        "\n",
        "### Observation Space:\n",
        "- A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "To make learning efficient, we can't just throw raw RGB frames at the network. The following section handles State Representation:\n",
        "\n",
        "- **Preprocessing:** We convert images to grayscale and resize them to $84 \\times 84$ to reduce the computational load.\n",
        "- **Frame Skipping:** The agent doesn't need to see every single frame to understand movement. We skip frames to speed up training.\n",
        "- **Frame Stacking:** A single static image doesn't show velocity. By stacking the 4 most recent frames, we provide the network with temporal context (motion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hRCIHWCMc_QQ"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img):\n",
        "  img = cv2.resize(img, dsize=(84, 84))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the Environment Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h4ArKIrdu2c"
      },
      "outputs": [],
      "source": [
        "class CarEnvironment(gym.Wrapper):\n",
        "  def __init__(self, env, skip_frames=3, stack_frames=4, no_operation=50, **kwargs):\n",
        "    super().__init__(env, **kwargs)\n",
        "    self._no_operation = no_operation\n",
        "    self._skip_frames = skip_frames # We choose an action and repeat it for N (4) frames\n",
        "    self._stack_frames = stack_frames # We stack N (4) frames to be able to infer velocity information\n",
        "\n",
        "  def reset(self):\n",
        "    observation, info = self.env.reset()\n",
        "\n",
        "    for i in range(self._no_operation):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
        "    return self.stack_state, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0\n",
        "    for i in range(self._skip_frames):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
        "    return self.stack_state, total_reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Brain: Convolutional Neural Network (CNN)\n",
        "\n",
        "Standard Q-Learning uses a table to store values, but our state space is too vast for that. Instead, we use a Function Approximator: a CNN.\n",
        "\n",
        "- The Convolutional Layers act as feature extractors, identifying the edges of the track and the car's position.\n",
        "\n",
        "- The Fully Connected Layers map these visual features to Q-values for each of the 5 possible discrete actions (e.g., steer left, accelerate, brake).\n",
        "\n",
        "### Action Space\n",
        "If continuous there are 3 actions :\n",
        "\n",
        "    0: steering, (-1 is full left, +1 is full right)\n",
        "    1: gas\n",
        "    2: braking\n",
        "\n",
        "If discrete there are 5 actions:\n",
        "\n",
        "    0: do nothing\n",
        "    1: steer right\n",
        "    2: steer left\n",
        "    3: gas\n",
        "    4: brake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mk4BxqWQhKjQ"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self._n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self._n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_channels),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self._n_features))\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stabilizing Learning: Replay Memory\n",
        "\n",
        "One major challenge in RL is that consecutive experiences are highly correlated (e.g., if you are turning left now, you are likely still turning left in the next frame). This breaks the \"Independent and Identically Distributed\" (IID) assumption required for neural network training.\n",
        "- We use Experience Replay to store transitions $(s, a, s', r)$ in a buffer.\n",
        "- During training, we sample random batches, which breaks the correlation between samples and leads to more stable convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2TqOGwEXkNYA"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The DQN Algorithm Logic\n",
        "This is the core of the agent. It implements two critical DQN features:\n",
        "1. **Epsilon-Greedy Strategy:** To balance Exploration (trying new things) and Exploitation (using what we know). Notice how eps_threshold decays over time.\n",
        "2. **Target Network:** To prevent the \"moving target\" problem, we use two identical networks. The network is updated every step, while the target_network stays fixed and is only updated every $C$ steps to provide stable ground-truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzoY0D21Dno"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "  def __init__(self, action_space, batch_size=256, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.001):\n",
        "    self._n_observation = 4\n",
        "    self._n_actions = 5\n",
        "    self._action_space = action_space\n",
        "    self._batch_size = batch_size\n",
        "    self._gamma = gamma\n",
        "    self._eps_start = eps_start\n",
        "    self._eps_end = eps_end\n",
        "    self._eps_decay = eps_decay\n",
        "    self._lr = lr\n",
        "    self._total_steps = 0\n",
        "    self._evaluate_loss = []\n",
        "    self.network = CNN(self._n_observation, self._n_actions).to(device)\n",
        "    self.target_network = CNN(self._n_observation, self._n_actions).to(device)\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "    self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
        "    self._memory = ReplayMemory(10000)\n",
        "\n",
        "  \"\"\"\n",
        "  This function is called during training & evaluation phase when the agent\n",
        "  interact with the environment and needs to select an action.\n",
        "\n",
        "  (1) Exploitation: This function feeds the neural network a state\n",
        "  and then it selects the action with the highest Q-value.\n",
        "  (2) Evaluation mode: This function feeds the neural network a state\n",
        "  and then it selects the action with the highest Q'-value.\n",
        "  (3) Exploration mode: It randomly selects an action through sampling\n",
        "\n",
        "  Q -> network (policy)\n",
        "  Q'-> target network (best policy)\n",
        "  \"\"\"\n",
        "  def select_action(self, state, evaluation_phase=False):\n",
        "\n",
        "    # Generating a random number for eploration vs exploitation\n",
        "    sample = random.random()\n",
        "\n",
        "    # Calculating the threshold - the more steps the less exploration we do\n",
        "    eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
        "    self._total_steps += 1\n",
        "\n",
        "    if evaluation_phase:\n",
        "      with torch.no_grad():\n",
        "        return self.target_network(state).max(1).indices.view(1, 1)\n",
        "    elif sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "        return self.network(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "      return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "  \"\"\"\n",
        "  This function trains the agent\n",
        "  \"\"\"\n",
        "  def train(self):\n",
        "\n",
        "    if len(self._memory) < self._batch_size:\n",
        "        return\n",
        "\n",
        "    # Initializing our memory\n",
        "    transitions = self._memory.sample(self._batch_size)\n",
        "\n",
        "    # Initializing our batch\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Saving in a new tensor all the indices of the states that are non terminal\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "\n",
        "    # Saving in a new tensor all the non final next states\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Feeding our Q network the batch with states and then we gather the Q values of the selected actions\n",
        "    state_action_values = self.network(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # We then, for every state in the batch that is NOT final, we pass it in the target network to get the Q'-values and choose the max one\n",
        "    next_state_values = torch.zeros(self._batch_size, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
        "\n",
        "    # Computing the expecting values with: reward + gamma * max(Q')\n",
        "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
        "\n",
        "    # Defining our loss criterion\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Updating with back propagation\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self._evaluate_loss.append(loss.item())\n",
        "\n",
        "    return\n",
        "\n",
        "  def copy_weights(self):\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "\n",
        "  def get_loss(self):\n",
        "    return self._evaluate_loss\n",
        "\n",
        "  def save_model(self, i):\n",
        "    torch.save(self.target_network.state_dict(), f'model_weights/model_weights_{i}.pth')\n",
        "\n",
        "  def load_model(self, i):\n",
        "    self.target_network.load_state_dict(torch.load(f'model_weights/model_weights_{i}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SPV8PdRwA5h4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "20 episodes done\n",
            "30 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "40 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "50 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "60 episodes done\n",
            "Finished the lap successfully!\n",
            "70 episodes done\n",
            "Finished the lap successfully!\n",
            "80 episodes done\n",
            "90 episodes done\n",
            "100 episodes done\n",
            "110 episodes done\n",
            "120 episodes done\n",
            "Finished the lap successfully!\n",
            "130 episodes done\n",
            "140 episodes done\n",
            "150 episodes done\n",
            "160 episodes done\n",
            "170 episodes done\n",
            "Finished the lap successfully!\n",
            "180 episodes done\n",
            "Finished the lap successfully!\n",
            "190 episodes done\n",
            "200 episodes done\n",
            "Finished the lap successfully!\n",
            "210 episodes done\n",
            "220 episodes done\n",
            "Finished the lap successfully!\n",
            "230 episodes done\n",
            "240 episodes done\n",
            "250 episodes done\n",
            "260 episodes done\n",
            "270 episodes done\n",
            "Finished the lap successfully!\n",
            "280 episodes done\n",
            "Finished the lap successfully!\n",
            "290 episodes done\n",
            "Finished the lap successfully!\n",
            "300 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "310 episodes done\n",
            "320 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "330 episodes done\n",
            "Finished the lap successfully!\n",
            "340 episodes done\n",
            "350 episodes done\n",
            "Finished the lap successfully!\n",
            "360 episodes done\n",
            "370 episodes done\n",
            "380 episodes done\n",
            "390 episodes done\n",
            "Finished the lap successfully!\n",
            "400 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n",
            "410 episodes done\n",
            "Finished the lap successfully!\n",
            "Finished the lap successfully!\n"
          ]
        }
      ],
      "source": [
        "rewards_per_episode = []\n",
        "episode_duration = []\n",
        "average_episode_loss = []\n",
        "\n",
        "episodes = 1000\n",
        "C = 5\n",
        "\n",
        "env = gym.make('CarRacing-v3', continuous=False)\n",
        "n_actions = env.action_space\n",
        "agent = DQN(n_actions)\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "\n",
        "  if episode % 10 == 0:\n",
        "    print(f\"{episode} episodes done\")\n",
        "\n",
        "  env = gym.make('CarRacing-v3', continuous=False)\n",
        "  env = CarEnvironment(env)\n",
        "\n",
        "  state, info = env.reset()\n",
        "\n",
        "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "  episode_total_reward = 0\n",
        "\n",
        "  for t in count():\n",
        "    action = agent.select_action(state)\n",
        "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "    reward = torch.tensor([reward], device=device)\n",
        "    episode_total_reward += reward\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if terminated:\n",
        "      next_state = None\n",
        "      print(\"Finished the lap successfully!\")\n",
        "    else:\n",
        "      next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    agent._memory.push(state, action, next_state, reward)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    agent.train()\n",
        "\n",
        "    if done:\n",
        "      if agent._memory.__len__() >= 128:\n",
        "        episode_duration.append(t + 1)\n",
        "        rewards_per_episode.append(episode_total_reward)\n",
        "        ll = agent.get_loss()\n",
        "        average_episode_loss.append(sum(ll) / len(ll))\n",
        "      break\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "      agent.save_model(episode)\n",
        "      with open('statistics.pkl', 'wb') as f:\n",
        "        pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)\n",
        "\n",
        "\n",
        "  if episode % C == 0:\n",
        "    agent.copy_weights()\n",
        "\n",
        "agent.save_model(episodes)\n",
        "with open('statistics.pkl', 'wb') as f:\n",
        "  pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR_KpaPU9iUd"
      },
      "source": [
        "# Evaluation of the agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5cVtbphvX7x"
      },
      "outputs": [],
      "source": [
        "def plot_statistics(x, y, title, x_axis, y_axis):\n",
        "  plt.plot(x, y)\n",
        "  plt.xlabel(x_axis)\n",
        "  plt.ylabel(y_axis)\n",
        "  plt.title(title)\n",
        "  plt.grid(True)\n",
        "  plt.savefig(f'{title}.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfNnnJUfemOt"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "eval_env = CarEnvironment(eval_env)\n",
        "\n",
        "frames = []\n",
        "scores = 0\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "eval_env.np_random = np.random.default_rng(42)\n",
        "\n",
        "done, ret = False, 0\n",
        "\n",
        "while not done:\n",
        "    frames.append(eval_env.render())\n",
        "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    a = agent.select_action(s, evaluation_phase=True)\n",
        "    discrete_action = a.item() % 5\n",
        "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
        "    s = s_prime\n",
        "    ret += r\n",
        "    done = terminated or truncated\n",
        "    if terminated:\n",
        "      print(terminated)\n",
        "scores += ret\n",
        "\n",
        "\n",
        "def animate(imgs, video_name, _return=True):\n",
        "    import cv2\n",
        "    import os\n",
        "    import string\n",
        "    import random\n",
        "\n",
        "    if video_name is None:\n",
        "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
        "    height, width, layers = imgs[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
        "\n",
        "    for img in imgs:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        video.write(img)\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi1bJ3j_k6U3"
      },
      "outputs": [],
      "source": [
        "animate(frames, None)\n",
        "\n",
        "with open('statistics.pkl', 'rb') as f:\n",
        "    data_tuple = pickle.load(f)\n",
        "\n",
        "episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n",
        "\n",
        "x = [k for k in range(episodes)]\n",
        "\n",
        "plot_statistics(x, rewards_per_episode, \"Rewards for every episode\", \"Episode\", \"Reward\")\n",
        "plot_statistics(x, average_episode_loss, \"Average loss for every episode\", \"Episode\", \"Average Loss\")\n",
        "plot_statistics(x, episode_duration, \"Duration (in steps) for every episode\", \"Episode\", \"Duration\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl_gymnasium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
