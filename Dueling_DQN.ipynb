{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSPpKLU97MSm"
      },
      "source": [
        "# Installing the important libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pT7fTAWKXHl3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nconda install -c conda-forge swig -y\\nconda install -c conda-forge box2d-py -y\\npython -m pip install --upgrade pip setuptools wheel\\npython -m pip install gymnasium[box2d]\\n'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "conda install -c conda-forge swig -y\n",
        "conda install -c conda-forge box2d-py -y\n",
        "python -m pip install --upgrade pip setuptools wheel\n",
        "python -m pip install gymnasium[box2d]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqrTw6l7Uis"
      },
      "source": [
        "# Importing the important modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "G41GVG9LcXiv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhOwyKe47a3d"
      },
      "source": [
        "# Implementation of the Car Racing game from Gymnasium using Deep Q Learning with CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Dueling DQN vs. Standard DQN\n",
        "\n",
        "In traditional Reinforcement Learning, the agent tries to learn the Action-Value function, denoted as $Q(s, a)$. This value represents the expected total reward the agent will receive by taking action $a$ in state $s$. However, in many states, the specific action chosen does not significantly impact the outcome. For example, in a car racing game, whether the car steers slightly left or right while on a straight stretch of road might not change the overall \"value\" of that state.\n",
        "\n",
        "### The Standard DQN Approach\n",
        "\n",
        "In a standard Deep Q-Network (DQN), the neural network treats every state-action pair as a unique entity to be learned. The architecture consists of:\n",
        "\n",
        "- **Convolutional Layers:** Extract spatial features from the game frames.\n",
        "\n",
        "- **Fully Connected Layers:** Map these features directly to a set of $Q(s, a)$ valuesâ€”one for every possible move (e.g., steer left, steer right, gas, brake).\n",
        "\n",
        "### The Dueling DQN Innovation\n",
        "\n",
        "The Dueling DQN architecture changes how the network calculates these values by splitting the final layers into two separate streams:\n",
        "\n",
        "- **State Value Stream ($V(s)$):** This stream predicts how \"good\" it is to be in a certain state, regardless of which action is taken.\n",
        "\n",
        "- **Advantage Stream ($A(s, a)$):** This stream predicts the relative importance (advantage) of each action compared to the others in that specific state.\n",
        "\n",
        "By separating these two, the agent can learn which states are valuable even if it hasn't explored every single action in those states yet. This makes the learning process much more efficient.\n",
        "\n",
        "### The Aggregation Layer\n",
        "\n",
        "To get the final Q-values used for decision-making, we combine the two streams using a specific formula:\n",
        "\n",
        "$$Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|A|} \\sum_{a'} A(s, a') \\right)$$\n",
        "\n",
        "Why subtract the mean? \n",
        "\n",
        "We subtract the average advantage to \"center\" the values. This ensures that the $V(s)$ stream truly captures the average value of the state, while the $A(s, a)$ stream focuses purely on the differences between actions.Key \n",
        "\n",
        "### Differences between Dueling and normal DQN\n",
        "\n",
        "\n",
        "In the Standard DQN, the agent learns the \"Quality\" of a state-action pair $Q(s, a)$ as a single value. In the Dueling DQN, the network is forced to learn two separate things:\n",
        "\n",
        "- State Value ($V$): How good is it to be in this position?\n",
        "- Advantage ($A$): How much better is \"Steer Left\" compared to \"Steer Right\" in this specific moment?T\n",
        "\n",
        "his separation is particularly effective for racing. For example, when driving at high speeds on a straight stretch of track, the \"Value\" of being on the road is very high, regardless of whether the agent chooses to steer slightly left or right. The Dueling DQN recognizes this redundancy, leading to more stable and efficient training.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Preprocessing\n",
        "\n",
        "### Observation Space:\n",
        "- A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "To make learning efficient, we can't just throw raw RGB frames at the network. The following section handles State Representation:\n",
        "\n",
        "- **Preprocessing:** We convert images to grayscale and resize them to $84 \\times 84$ to reduce the computational load.\n",
        "- **Frame Skipping:** The agent doesn't need to see every single frame to understand movement. We skip frames to speed up training.\n",
        "- **Frame Stacking:** A single static image doesn't show velocity. By stacking the 4 most recent frames, we provide the network with temporal context (motion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hRCIHWCMc_QQ"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img):\n",
        "  img = cv2.resize(img, dsize=(84, 84))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the Environment Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1h4ArKIrdu2c"
      },
      "outputs": [],
      "source": [
        "class CarEnvironment(gym.Wrapper):\n",
        "  def __init__(self, env, skip_frames=3, stack_frames=4, no_operation=50, **kwargs):\n",
        "    super().__init__(env, **kwargs)\n",
        "    self._no_operation = no_operation\n",
        "    self._skip_frames = skip_frames\n",
        "    self._stack_frames = stack_frames\n",
        "\n",
        "  def reset(self):\n",
        "    observation, info = self.env.reset()\n",
        "\n",
        "    for i in range(self._no_operation):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
        "    return self.stack_state, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0\n",
        "    for i in range(self._skip_frames):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
        "    return self.stack_state, total_reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Brain: Dueling Newtork Architecture (CNN)\n",
        "\n",
        "Unlike a standard DQN that outputs $Q(s, a)$ directly, a Dueling DQN splits the network into two streams:\n",
        "\n",
        "- **Value Stream ($V(s)$):** Estimates how good it is to be in a certain state.\n",
        "- **Advantage Stream ($A(s, a)$):** Estimates the relative benefit of each action compared to others in that state.\n",
        "\n",
        "These are combined using the formula:$$Q(s, a) = V(s) + (A(s, a) - \\frac{1}{|A|} \\sum A(s, a'))$$\n",
        "This separation allows the agent to learn which states are valuable without having to learn the effect of each action for every single state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Mk4BxqWQhKjQ"
      },
      "outputs": [],
      "source": [
        "class DuelingCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "    self.v = nn.Sequential(\n",
        "      nn.Linear(self.n_features, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "    self.a = nn.Sequential(\n",
        "        nn.Linear(self.n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, self.out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self.n_features))\n",
        "    val = self.v(x)\n",
        "    adv = self.a(x)\n",
        "    q = val + (adv - adv.mean())\n",
        "\n",
        "    return q\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stabilizing Learning: Replay Memory\n",
        "\n",
        "One major challenge in RL is that consecutive experiences are highly correlated (e.g., if you are turning left now, you are likely still turning left in the next frame). This breaks the \"Independent and Identically Distributed\" (IID) assumption required for neural network training.\n",
        "- We use Experience Replay to store transitions $(s, a, s', r)$ in a buffer.\n",
        "- During training, we sample random batches, which breaks the correlation between samples and leads to more stable convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2TqOGwEXkNYA"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The DQN Algorithm Logic\n",
        "This is the core of the agent, it is the function responsible for selecting an action based on the observation state. It implements two critical DQN features:\n",
        "1. **Epsilon-Greedy Strategy:** To balance Exploration (trying new things) and Exploitation (using what we know). Notice how eps_threshold decays over time.\n",
        "2. **Target Network:** To prevent the \"moving target\" problem, we use two identical networks. The network is updated every step, while the target_network stays fixed and is only updated every $C$ steps to provide stable ground-truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzoY0D21Dno"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "  def __init__(self, action_space, batch_size=256, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.001):\n",
        "    self._n_observation = 4 # Stacked frames\n",
        "    self._n_actions = 5 # Possible discretized actions\n",
        "    self._action_space = action_space\n",
        "    self._batch_size = batch_size # Batch size for training: Higher batch size means more stable training but requires more memory\n",
        "    self._gamma = gamma # Discount factor for future rewards. The closer to 1 it is, the more the agent will consider future rewards\n",
        "    self._eps_start = eps_start # Initial exploration probability\n",
        "    self._eps_end = eps_end # Final exploration probability\n",
        "    self._eps_decay = eps_decay # Decay rate for exploration probability\n",
        "    self._lr = lr # Learning rate for the optimizer: Higher learning rate means faster learning but can lead to instability\n",
        "    self._total_steps = 0\n",
        "    self._evaluate_loss = []\n",
        "    self.network = DuelingCNN(self._n_observation, self._n_actions).to(device) # Policy network\n",
        "    self.target_network = DuelingCNN(self._n_observation, self._n_actions).to(device) # Target network\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "    self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
        "    self._memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "  def select_action(self, state, evaluation_phase=False):\n",
        "    sample = random.random()\n",
        "    eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
        "    self._total_steps += 1\n",
        "    if evaluation_phase:\n",
        "      with torch.no_grad():\n",
        "        return self.target_network(state).max(1).indices.view(1, 1)\n",
        "    elif sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "        return self.network(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "      return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    if len(self._memory) < self._batch_size:\n",
        "        return\n",
        "    transitions = self._memory.sample(self._batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    state_action_values = self.network(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(self._batch_size, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
        "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self._evaluate_loss.append(loss.item())\n",
        "\n",
        "    return\n",
        "\n",
        "  def copy_weights(self):\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "\n",
        "  def get_loss(self):\n",
        "    return self._evaluate_loss\n",
        "\n",
        "  def save_model(self, i):\n",
        "    os.makedirs('Dueling_DQN_model_weights', exist_ok=True)\n",
        "    torch.save(self.target_network.state_dict(), f'Dueling_DQN_model_weights/model_weights_{i}.pth')\n",
        "    path = f'Dueling_DQN_model_weights/model_weights_{i}.pth'\n",
        "\n",
        "  def load_model(self, i):\n",
        "    path = f'Dueling_DQN_model_weights/model_weights_{i}.pth'\n",
        "    legacy = f'model_weights_{i}.pth'\n",
        "\n",
        "    # Try the current path first\n",
        "    if os.path.exists(path):\n",
        "      self.target_network.load_state_dict(torch.load(path, map_location=device))\n",
        "      return\n",
        "\n",
        "    # Then try the legacy filename in cwd\n",
        "    if os.path.exists(legacy):\n",
        "      self.target_network.load_state_dict(torch.load(legacy, map_location=device))\n",
        "      return\n",
        "\n",
        "    # If neither exists, raise\n",
        "    raise FileNotFoundError(f'Could not find model file {path} or {legacy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SPV8PdRwA5h4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Dueling_CNN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m env = gym.make(\u001b[33m'\u001b[39m\u001b[33mCarRacing-v3\u001b[39m\u001b[33m'\u001b[39m, continuous=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m n_actions = env.action_space\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m agent = \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, episodes + \u001b[32m1\u001b[39m):\n\u001b[32m     14\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mDQN.__init__\u001b[39m\u001b[34m(self, action_space, batch_size, gamma, eps_start, eps_end, eps_decay, lr)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m._total_steps = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m._evaluate_loss = []\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mself\u001b[39m.network = \u001b[43mDueling_CNN\u001b[49m(\u001b[38;5;28mself\u001b[39m._n_observation, \u001b[38;5;28mself\u001b[39m._n_actions).to(device) \u001b[38;5;66;03m# Policy network\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.target_network = Dueling_CNN(\u001b[38;5;28mself\u001b[39m._n_observation, \u001b[38;5;28mself\u001b[39m._n_actions).to(device) \u001b[38;5;66;03m# Target network\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.target_network.load_state_dict(\u001b[38;5;28mself\u001b[39m.network.state_dict())\n",
            "\u001b[31mNameError\u001b[39m: name 'Dueling_CNN' is not defined"
          ]
        }
      ],
      "source": [
        "rewards_per_episode = []\n",
        "episode_duration = []\n",
        "average_episode_loss = []\n",
        "\n",
        "episodes = 500\n",
        "C = 5\n",
        "\n",
        "env = gym.make('CarRacing-v3', continuous=False)\n",
        "n_actions = env.action_space\n",
        "agent = DQN(n_actions)\n",
        "\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "  if episode % 10 == 0:\n",
        "    print(f\"{episode} episodes done\")\n",
        "\n",
        "  env = gym.make('CarRacing-v3', continuous=False)\n",
        "  env = CarEnvironment(env)\n",
        "\n",
        "  state, info = env.reset()\n",
        "\n",
        "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "  episode_total_reward = 0.0\n",
        "\n",
        "  for t in count():\n",
        "    action = agent.select_action(state)\n",
        "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "    # keep a float copy for logging/plotting and a tensor for storage\n",
        "    episode_total_reward += float(reward)\n",
        "    reward_tensor = torch.tensor([reward], device=device)\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if terminated:\n",
        "      next_state = None\n",
        "      print(\"Finished the lap successfully!\")\n",
        "    else:\n",
        "      next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    agent._memory.push(state, action, next_state, reward_tensor)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    agent.train()\n",
        "\n",
        "    if done:\n",
        "      if agent._memory.__len__() >= 128:\n",
        "        episode_duration.append(t + 1)\n",
        "        rewards_per_episode.append(episode_total_reward)\n",
        "        ll = agent.get_loss()\n",
        "        if len(ll) > 0:\n",
        "          average_episode_loss.append(sum(ll) / len(ll))\n",
        "        else:\n",
        "          average_episode_loss.append(0.0)\n",
        "      break\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "      agent.save_model(episode)\n",
        "      with open('statistics.pkl', 'wb') as f:\n",
        "        pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)\n",
        "\n",
        "\n",
        "  if episode % C == 0:\n",
        "    agent.copy_weights()\n",
        "\n",
        "agent.save_model(episodes)\n",
        "with open('statistics.pkl', 'wb') as f:\n",
        "  pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR_KpaPU9iUd"
      },
      "source": [
        "# Evaluation of the agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5cVtbphvX7x"
      },
      "outputs": [],
      "source": [
        "def plot_statistics(x, y, title, x_axis, y_axis):\n",
        "  import numpy as _np\n",
        "  # Ensure y is numeric floats (handles any tensors or numeric types)\n",
        "  y = _np.array([float(v) for v in y])\n",
        "  x = _np.array(x)\n",
        "  plt.plot(x, y)\n",
        "  plt.xlabel(x_axis)\n",
        "  plt.ylabel(y_axis)\n",
        "  plt.title(title)\n",
        "  plt.grid(True)\n",
        "  plt.savefig(f'{title}.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfNnnJUfemOt"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v3', continuous=False, render_mode='rgb_array')\n",
        "eval_env = CarEnvironment(eval_env)\n",
        "\n",
        "frames = []\n",
        "scores = 0\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "done, ret = False, 0\n",
        "\n",
        "while not done:\n",
        "    frames.append(eval_env.render())\n",
        "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    a = agent.select_action(s, evaluation_phase=True)\n",
        "    discrete_action = a.item() % 5\n",
        "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
        "    s = s_prime\n",
        "    ret += r\n",
        "    done = terminated or truncated\n",
        "    if terminated:\n",
        "      print(terminated)\n",
        "scores += ret\n",
        "\n",
        "\n",
        "def animate(imgs, video_name, _return=True):\n",
        "    import cv2\n",
        "    import os\n",
        "    import string\n",
        "    import random\n",
        "\n",
        "    if video_name is None:\n",
        "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
        "    height, width, layers = imgs[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
        "\n",
        "    for img in imgs:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        video.write(img)\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi1bJ3j_k6U3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OpenCV: FFMPEG: tag 0x30395056/'VP90' is not supported with codec id 167 and format 'webm / WebM'\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n\u001b[32m      8\u001b[39m x = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes)]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mplot_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards_per_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRewards for every episode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpisode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m plot_statistics(x, average_episode_loss, \u001b[33m\"\u001b[39m\u001b[33mAverage loss for every episode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEpisode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAverage Loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m plot_statistics(x, episode_duration, \u001b[33m\"\u001b[39m\u001b[33mDuration (in steps) for every episode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEpisode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDuration\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mplot_statistics\u001b[39m\u001b[34m(x, y, title, x_axis, y_axis)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_statistics\u001b[39m(x, y, title, x_axis, y_axis):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m   plt.xlabel(x_axis)\n\u001b[32m      4\u001b[39m   plt.ylabel(y_axis)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/matplotlib/axes/_base.py:484\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) == \u001b[32m2\u001b[39m:\n\u001b[32m    483\u001b[39m     x = _check_1d(xy[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     y = \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    486\u001b[39m     x, y = index_of(xy[-\u001b[32m1\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/matplotlib/cbook.py:1368\u001b[39m, in \u001b[36m_check_1d\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[32m   1363\u001b[39m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[32m   1364\u001b[39m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1367\u001b[39m         \u001b[38;5;28mlen\u001b[39m(x.shape) < \u001b[32m1\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/numpy/core/shape_base.py:65\u001b[39m, in \u001b[36matleast_1d\u001b[39m\u001b[34m(*arys)\u001b[39m\n\u001b[32m     63\u001b[39m res = []\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     ary = \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ary.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m         result = ary.reshape(\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "animate(frames, None)\n",
        "\n",
        "with open('statistics.pkl', 'rb') as f:\n",
        "    data_tuple = pickle.load(f)\n",
        "\n",
        "episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n",
        "\n",
        "# Plot using actual data lengths to avoid mismatched sizes\n",
        "plot_statistics(list(range(len(rewards_per_episode))), rewards_per_episode, \"Rewards for every episode\", \"Episode\", \"Reward\")\n",
        "plot_statistics(list(range(len(average_episode_loss))), average_episode_loss, \"Average loss for every episode\", \"Episode\", \"Average Loss\")\n",
        "plot_statistics(list(range(len(episode_duration))), episode_duration, \"Duration (in steps) for every episode\", \"Episode\", \"Duration\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl_gymnasium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
